{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = '../../twitter-swisscom/sample.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns_names = ['id', 'userId', 'createdAt', 'text', 'longitude', 'latitude', 'placeId', 'inReplyTo', 'source',\n",
    "                 'truncated', 'placeLatitude', 'placeLongitude', 'sourceName', 'sourceUrl', 'userName', 'screenName',\n",
    "                 'followersCount', 'friendsCount', 'statusesCount', 'userLocation']\n",
    "\n",
    "# the columns that interest us for the density map\n",
    "columns_to_keep = ['id', 'createdAt', 'placeLatitude', 'placeLongitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set to None to get all the records\n",
    "num_rows = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_data = pd.read_csv(data_path, sep='\\t', encoding='utf-8', escapechar='\\\\', quoting=csv.QUOTE_NONE,\n",
    "                      header=None, na_values='N', nrows=num_rows)\n",
    "\n",
    "# give labels to the columns\n",
    "df_data.columns = columns_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_data = df_data[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                0\n",
       "createdAt         0\n",
       "placeLatitude     0\n",
       "placeLongitude    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if we have NaNs in any column\n",
    "df_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_data['year'] = pd.DatetimeIndex(df_data['createdAt']).year\n",
    "df_data['month'] = pd.DatetimeIndex(df_data['createdAt']).month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_unix_time(record):\n",
    "    datetime_index = pd.DatetimeIndex([datetime(record['year'], record['month'], 1)])\n",
    "    unix_time_index = datetime_index.astype(np.int64) // 10**6\n",
    "    return unix_time_index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_data['unix_time'] = df_data.apply(convert_to_unix_time, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_data.drop(['createdAt', 'year', 'month'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Geolocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from shapely.geometry import Point, shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read geoJSON file with canton names and IDs\n",
    "path_cantons = '../res/topo/ch-cantons-geo.json'\n",
    "\n",
    "with open(path_cantons) as file:\n",
    "    cantons_json = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_canton_id(record):\n",
    "    point = Point(record['placeLongitude'], record['placeLatitude'])\n",
    "    \n",
    "    for feature in cantons_json['features']:\n",
    "        if(feature['geometry']['type'] == 'Polygon'):\n",
    "            polygon = shape(feature['geometry'])\n",
    "            if polygon.contains(point):\n",
    "                return feature['properties']['id']\n",
    "        elif(feature['geometry']['type'] == 'MultiPolygon'):\n",
    "            multipolygon = shape(feature['geometry'])\n",
    "            for polygon in multipolygon:\n",
    "                if polygon.contains(point):\n",
    "                    return feature['properties']['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for each record, get the ID corresponding to the canton\n",
    "df_data['canton_id'] = df_data.apply(get_canton_id, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tweets which location is not in CH: 2737\n"
     ]
    }
   ],
   "source": [
    "print('number of tweets which location is not in CH:', df_data['canton_id'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop records where the location didn't correspond to a place in CH\n",
    "df_data.dropna(subset=['canton_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping Tweets by Year and Canton/Town"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grouped_year_canton = df_data.groupby(['unix_time', 'canton_id']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unix_time      canton_id\n",
       "1472688000000  1.0          1265\n",
       "               2.0           525\n",
       "               3.0           372\n",
       "               4.0             5\n",
       "               5.0            31\n",
       "               6.0           116\n",
       "               7.0            12\n",
       "               8.0             2\n",
       "               9.0            47\n",
       "               10.0          139\n",
       "               11.0           57\n",
       "               12.0          218\n",
       "               13.0           57\n",
       "               14.0           12\n",
       "               15.0            2\n",
       "               16.0           17\n",
       "               17.0          153\n",
       "               18.0           76\n",
       "               19.0          238\n",
       "               20.0           70\n",
       "               21.0          269\n",
       "               22.0          722\n",
       "               23.0          236\n",
       "               24.0          170\n",
       "               25.0         1233\n",
       "               26.0            9\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_year_canton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# grouped_year_town = df_data.groupby(['unix_time', 'town']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating JSON Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_json_file(grouped_dataframe, output_filename):\n",
    "    dates_list = list(grouped_dataframe.index.levels[0])\n",
    "    cantons_list = list(grouped_dataframe.index.levels[1])\n",
    "    \n",
    "    cantons = dict()\n",
    "    cantons['cantons'] = list()\n",
    "    \n",
    "    for date_index in range(len(dates_list)):\n",
    "        cantons['cantons'].append(dict())\n",
    "        cantons['cantons'][date_index]['date'] = int(dates_list[date_index])\n",
    "        cantons['cantons'][date_index]['data'] = list()\n",
    "\n",
    "        for canton_index in range(len(cantons_list)):\n",
    "            cantons['cantons'][date_index]['data'].append(dict())\n",
    "            cantons['cantons'][date_index]['data'][canton_index]['id'] = int(cantons_list[canton_index])\n",
    "            cantons['cantons'][date_index]['data'][canton_index]['nbr'] = int(grouped_year_canton[(dates_list[date_index],\n",
    "                                                                                                   cantons_list[canton_index])])\n",
    "            \n",
    "    with open(output_filename, 'w') as file:\n",
    "        json.dump(cantons, file)\n",
    "        \n",
    "    return cantons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cantons_json = create_json_file(grouped_year_canton, '../res/density/canton_density.json')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
