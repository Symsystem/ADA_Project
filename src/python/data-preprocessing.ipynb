{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = '../twitter-swisscom/sample.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns_names = ['id', 'userId', 'createdAt', 'text', 'longitude', 'latitude', 'placeId', 'inReplyTo', 'source',\n",
    "                 'truncated', 'placeLatitude', 'placeLongitude', 'sourceName', 'sourceUrl', 'userName', 'screenName',\n",
    "                 'followersCount', 'friendsCount', 'statusesCount', 'userLocation']\n",
    "\n",
    "# the columns that interest us for the density map\n",
    "columns_to_keep = ['id', 'createdAt', 'placeLatitude', 'placeLongitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set to None to get all the records\n",
    "num_rows = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_data = pd.read_csv(data_path, sep='\\t', encoding='utf-8', escapechar='\\\\', quoting=csv.QUOTE_NONE,\n",
    "                      header=None, na_values='N', nrows=num_rows)\n",
    "\n",
    "# give labels to the columns\n",
    "df_data.columns = columns_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_data = df_data[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check if we have NaNs in any column\n",
    "df_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_data['year'] = pd.DatetimeIndex(df_data['createdAt']).year\n",
    "df_data['month'] = pd.DatetimeIndex(df_data['createdAt']).month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_unix_time(record):\n",
    "    datetime_index = pd.DatetimeIndex([datetime(record['year'], record['month'], 1)])\n",
    "    unix_time_index = datetime_index.astype(np.int64) // 10**6\n",
    "    return unix_time_index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_data['unix_time'] = df_data.apply(convert_to_unix_time, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_data.drop(['createdAt', 'year', 'month'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Geolocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from geopy.geocoders import Nominatim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_locations(dataframe):\n",
    "    num_non_ch = 0\n",
    "    \n",
    "    geolocator = Nominatim()\n",
    "    \n",
    "    for index, row in dataframe.iterrows():\n",
    "        location = geolocator.reverse((row['placeLatitude'], row['placeLongitude']))\n",
    "        \n",
    "        # print(location.raw)\n",
    "        \n",
    "        if location.raw['address']['country_code'] == \"ch\":\n",
    "            dataframe.set_value(index, 'canton', location.raw['address']['state'])\n",
    "            if 'city' in location.raw['address']:\n",
    "                dataframe.set_value(index, 'town', location.raw['address']['city'])\n",
    "            elif 'town' in location.raw['address']:\n",
    "                dataframe.set_value(index, 'town', location.raw['address']['town'])\n",
    "            else:\n",
    "                dataframe.set_value(index, 'town', location.raw['address']['village'])\n",
    "        else:\n",
    "            num_non_ch += 1\n",
    "    \n",
    "    print('number of tweets which location is not in CH:', num_non_ch)\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_data = get_locations(df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop records where the location didn't correspond to a place in CH\n",
    "df_data.dropna(subset=['canton'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read mapping between canton names and IDs\n",
    "cantons_ids = pd.read_json(path_or_buf='cantons_mapping.json', orient='records', typ='series')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_canton_id(record):\n",
    "    split_canton = re.split(' - | ', record['canton'])\n",
    "    \n",
    "    for word in split_canton:\n",
    "        for canton_name in cantons_ids.index:\n",
    "            if word in canton_name:\n",
    "                return cantons_ids[canton_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for each record, get the ID corresponding to the canton\n",
    "df_data['canton_id'] = df_data.apply(get_canton_id, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping Tweets by Year and Canton/Town"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grouped_year_canton = df_data.groupby(['unix_time', 'canton_id']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grouped_year_canton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grouped_year_town = df_data.groupby(['unix_time', 'town']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating JSON Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_json_file(grouped_dataframe, output_filename):\n",
    "    dates_list = list(grouped_dataframe.index.levels[0])\n",
    "    cantons_list = list(grouped_dataframe.index.levels[1])\n",
    "    \n",
    "    cantons = dict()\n",
    "    cantons['cantons'] = list()\n",
    "    \n",
    "    for date_index in range(len(dates_list)):\n",
    "        cantons['cantons'].append(dict())\n",
    "        cantons['cantons'][date_index]['date'] = int(dates_list[date_index])\n",
    "        cantons['cantons'][date_index]['data'] = list()\n",
    "\n",
    "        for canton_index in range(len(cantons_list)):\n",
    "            cantons['cantons'][date_index]['data'].append(dict())\n",
    "            cantons['cantons'][date_index]['data'][canton_index]['id'] = int(cantons_list[canton_index])\n",
    "            cantons['cantons'][date_index]['data'][canton_index]['nbr'] = int(grouped_year_canton[(dates_list[date_index],\n",
    "                                                                                                   cantons_list[canton_index])])\n",
    "            \n",
    "    with open(output_filename, 'w') as file:\n",
    "        json.dump(cantons, file)\n",
    "        \n",
    "    return cantons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cantons_json = create_json_file(grouped_year_canton, 'canton_density.json')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
