{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = '../../twitter-swisscom/sample.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns_names = ['id', 'userId', 'createdAt', 'text', 'longitude', 'latitude', 'placeId', 'inReplyTo', 'source',\n",
    "                 'truncated', 'placeLatitude', 'placeLongitude', 'sourceName', 'sourceUrl', 'userName', 'screenName',\n",
    "                 'followersCount', 'friendsCount', 'statusesCount', 'userLocation']\n",
    "\n",
    "# the columns that interest us for the density map\n",
    "columns_to_keep = ['id', 'createdAt', 'placeLatitude', 'placeLongitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set to None to get all the records\n",
    "num_rows = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_data = pd.read_csv(data_path, sep='\\t', encoding='utf-8', escapechar='\\\\', quoting=csv.QUOTE_NONE,\n",
    "                      header=None, na_values='N', nrows=num_rows)\n",
    "\n",
    "# give labels to the columns\n",
    "df_data.columns = columns_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_data = df_data[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make sure all tweets are geolocated\n",
    "df_data.dropna(subset=['placeLatitude', 'placeLongitude'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_data['year'] = pd.DatetimeIndex(df_data['createdAt']).year\n",
    "df_data['month'] = pd.DatetimeIndex(df_data['createdAt']).month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_unix_time(record):\n",
    "    datetime_index = pd.DatetimeIndex([datetime(record['year'], record['month'], 1)])\n",
    "    unix_time_index = datetime_index.astype(np.int64) // 10**6\n",
    "    return unix_time_index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_data['unix_time'] = df_data.apply(convert_to_unix_time, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_data.drop(['createdAt', 'year', 'month'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Geolocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from shapely.geometry import Point, shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get the ID of cantons or municipalities, depending on the geoJSON file passed as an argument\n",
    "def get_id_from_geoJSON(record, filename):\n",
    "    point = Point(record['placeLongitude'], record['placeLatitude'])\n",
    "    \n",
    "    for feature in filename['features']:\n",
    "        if(feature['geometry']['type'] == 'Polygon'):\n",
    "            polygon = shape(feature['geometry'])\n",
    "            if polygon.contains(point):\n",
    "                return feature['properties']['id']\n",
    "        elif(feature['geometry']['type'] == 'MultiPolygon'):\n",
    "            multipolygon = shape(feature['geometry'])\n",
    "            for polygon in multipolygon:\n",
    "                if polygon.contains(point):\n",
    "                    return feature['properties']['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read geoJSON file with canton names and IDs\n",
    "path_cantons = '../res/topo/ch-cantons-geo.json'\n",
    "\n",
    "with open(path_cantons) as file:\n",
    "    cantons_json = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for each record, get the ID corresponding to the canton\n",
    "df_data['canton_id'] = df_data.apply(get_id_from_geoJSON, args=(cantons_json,), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tweets which location is not in CH: 2737\n"
     ]
    }
   ],
   "source": [
    "print('number of tweets which location is not in CH:', df_data['canton_id'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop records where the location didn't correspond to a place in CH\n",
    "df_data.dropna(subset=['canton_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read geoJSON file with municipalities names and IDs\n",
    "path_towns = '../res/topo/ch-municipalities-geo.json'\n",
    "\n",
    "with open(path_towns) as file:\n",
    "    towns_json = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for each record, get the ID corresponding to the municipality\n",
    "df_data['town_id'] = df_data.apply(get_id_from_geoJSON, args=(towns_json,), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping Tweets by Year and Canton/Municipality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grouped_year_canton = df_data.groupby(['unix_time', 'canton_id']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grouped_year_town = df_data.groupby(['unix_time', 'town_id']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating JSON Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_json_file(canton_municipality, grouped_dataframe, output_filename):\n",
    "    dates_list = list(grouped_dataframe.index.levels[0])\n",
    "    ids_list = list(grouped_dataframe.index.levels[1])\n",
    "    \n",
    "    if canton_municipality == 'c':\n",
    "        main_object = 'cantons'\n",
    "    else:\n",
    "        main_object = 'municipalities'\n",
    "    \n",
    "    json_file = dict()\n",
    "    json_file[main_object] = list()  \n",
    "    \n",
    "    for date_index in range(len(dates_list)):\n",
    "        json_file[main_object].append(dict())\n",
    "        json_file[main_object][date_index]['date'] = int(dates_list[date_index])\n",
    "        json_file[main_object][date_index]['data'] = list()\n",
    "\n",
    "        for id_index in range(len(ids_list)):\n",
    "            json_file[main_object][date_index]['data'].append(dict())\n",
    "            json_file[main_object][date_index]['data'][id_index]['id'] = int(ids_list[id_index])\n",
    "            json_file[main_object][date_index]['data'][id_index]['nbr'] = int(grouped_dataframe[(dates_list[date_index],\n",
    "                                                                                                 ids_list[id_index])])\n",
    "            \n",
    "    with open(output_filename, 'w') as file:\n",
    "        json.dump(json_file, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "create_json_file('c', grouped_year_canton, '../res/density/canton_density.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_json_file('m', grouped_year_town, '../res/density/municipality_density.json')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
