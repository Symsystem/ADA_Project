{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = '../../twitter-swisscom/twex.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columns_names = ['id', 'userId', 'createdAt', 'text', 'longitude', 'latitude', 'placeId', 'inReplyTo', 'source',\n",
    "                 'truncated', 'placeLatitude', 'placeLongitude', 'sourceName', 'sourceUrl', 'userName', 'screenName',\n",
    "                 'followersCount', 'friendsCount', 'statusesCount', 'userLocation']\n",
    "\n",
    "# the columns that interest us for the density map\n",
    "columns_to_keep = ['id', 'createdAt', 'placeLatitude', 'placeLongitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set to None to get all the records\n",
    "num_rows = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\InÃªs Valentim\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2723: DtypeWarning: Columns (0,1,2,3,5,6,8,9,10,11,12,13,14,15,16,19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df_data = pd.read_csv(data_path, sep='\\t', encoding='utf-8', escapechar='\\\\', quoting=csv.QUOTE_NONE,\n",
    "                      header=None, na_values='N', nrows=num_rows, names=columns_names)\n",
    "\n",
    "# give labels to the columns\n",
    "df_data.columns = columns_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                 object\n",
       "userId             object\n",
       "createdAt          object\n",
       "text               object\n",
       "longitude         float64\n",
       "latitude           object\n",
       "placeId            object\n",
       "inReplyTo         float64\n",
       "source             object\n",
       "truncated          object\n",
       "placeLatitude      object\n",
       "placeLongitude     object\n",
       "sourceName         object\n",
       "sourceUrl          object\n",
       "userName           object\n",
       "screenName         object\n",
       "followersCount     object\n",
       "friendsCount      float64\n",
       "statusesCount     float64\n",
       "userLocation       object\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_data = df_data[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28799234, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of tweets before dropping NaNs\n",
    "df_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make sure the ID of the tweets is a number\n",
    "df_data['numeric_id'] = df_data['id'].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make sure all tweets are geolocated and have an ID and a timestamp associated\n",
    "df_data.dropna(inplace=True)\n",
    "df_data.drop('numeric_id', axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20189524, 4)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of tweets after dropping NaNs\n",
    "df_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# according to the schema, dates should have the format 0000-00-00 00:00:00\n",
    "df_data = df_data[df_data['createdAt'].str.len() == 19]\n",
    "df_data['datetime'] = pd.to_datetime(df_data['createdAt'], errors='coerce')\n",
    "df_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_data['year'] = pd.DatetimeIndex(df_data['datetime']).year\n",
    "df_data['month'] = pd.DatetimeIndex(df_data['datetime']).month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# only consider tweets from 2010 or later\n",
    "df_data = df_data[df_data['year'] >= 2010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2014    5791960\n",
       "2013    5211698\n",
       "2015    3508696\n",
       "2016    2818604\n",
       "2012    2757776\n",
       "2011      83022\n",
       "2010      17733\n",
       "Name: year, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just to get an idea of the distribution of tweets per year\n",
    "df_data['year'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20189489, 7)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_data['dates'] = list(zip(df_data['month'], df_data['year']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "series_dates = pd.Series(df_data['dates'].unique())\n",
    "unix_times = pd.DataFrame({'dates': series_dates})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_unix_time(record):\n",
    "    datetime_index = pd.DatetimeIndex([datetime(record['dates'][1], record['dates'][0], 1)])\n",
    "    unix_time_index = datetime_index.astype(np.int64) // 10**6\n",
    "    return unix_time_index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unix_times['unix_time'] = unix_times.apply(convert_to_unix_time, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_data_dates = pd.merge(df_data, unix_times, how='inner', on='dates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_data_dates.drop(['createdAt', 'datetime', 'year', 'month', 'dates'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20189489, 4)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data_dates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_data_dates.to_csv(path_or_buf='clean_dates.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Geolocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_data_dates = pd.read_csv('clean_dates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20189489, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data_dates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from shapely.geometry import Point, shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_data_dates['coords'] = list(zip(df_data_dates['placeLongitude'], df_data_dates['placeLatitude']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a DataFrame with the unique pairs of coordinates\n",
    "series_coords = pd.Series(df_data_dates['coords'].unique())\n",
    "coords_cantons = pd.DataFrame({'coords': series_coords})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16413, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coords_cantons.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the ID of cantons or municipalities, depending on the geoJSON file passed as an argument\n",
    "def get_id_from_coords(record, filename):\n",
    "    point = Point(float(record['coords'][0]), float(record['coords'][1]))\n",
    "    \n",
    "    for feature in filename['features']:\n",
    "        if(feature['geometry']['type'] == 'Polygon'):\n",
    "            polygon = shape(feature['geometry'])\n",
    "            if polygon.contains(point):\n",
    "                return feature['properties']['id']\n",
    "        elif(feature['geometry']['type'] == 'MultiPolygon'):\n",
    "            multipolygon = shape(feature['geometry'])\n",
    "            for polygon in multipolygon:\n",
    "                if polygon.contains(point):\n",
    "                    return feature['properties']['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read geoJSON file with canton names and IDs\n",
    "path_cantons = '../res/topo/ch-cantons-geo.json'\n",
    "\n",
    "with open(path_cantons) as file:\n",
    "    cantons_json = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for each pair of coordinates, get the ID corresponding to the canton\n",
    "coords_cantons['canton_id'] = coords_cantons.apply(get_id_from_coords, args=(cantons_json,), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of coordinates outside CH: 6827\n"
     ]
    }
   ],
   "source": [
    "print('number of coordinates outside CH:', coords_cantons['canton_id'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# drop pairs of coordinates that are located outside of CH\n",
    "coords_cantons.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9586, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coords_cantons.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# merge the data based on the coordinates\n",
    "df_data_cantons = pd.merge(df_data_dates, coords_cantons, how='inner', on='coords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12705241, 6)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data_cantons.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_data_cantons.to_csv(path_or_buf='clean_cantons.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read geoJSON file with municipalities names and IDs\n",
    "path_towns = '../res/topo/ch-municipalities-geo.json'\n",
    "\n",
    "with open(path_towns) as file:\n",
    "    towns_json = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for each pair of coordinates, get the ID corresponding to the municipality\n",
    "coords_cantons['town_id'] = coords_cantons.apply(get_id_from_coords, args=(towns_json,), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9586, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coords_cantons.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge the data based on the coordinates\n",
    "df_data_towns = pd.merge(df_data_cantons, coords_cantons, how='inner', on='coords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12705241, 8)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data_towns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_data_towns.to_csv(path_or_buf='clean_towns.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping Tweets by Year and Canton/Municipality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grouped_year_canton = df_data_cantons.groupby(['unix_time', 'canton_id']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grouped_year_town = df_data_towns.groupby(['unix_time', 'town_id']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating JSON Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_json_file(canton_municipality, grouped_dataframe, output_filename):\n",
    "    dates_list = list(grouped_dataframe.index.levels[0])\n",
    "    \n",
    "    if canton_municipality == 'c':\n",
    "        main_object = 'cantons'\n",
    "    else:\n",
    "        main_object = 'municipalities'\n",
    "    \n",
    "    json_file = dict()\n",
    "    json_file[main_object] = list()  \n",
    "    \n",
    "    for date_index in range(len(dates_list)):\n",
    "        json_file[main_object].append(dict())\n",
    "        json_file[main_object][date_index]['date'] = int(dates_list[date_index])\n",
    "        json_file[main_object][date_index]['data'] = list()\n",
    "        \n",
    "        ids_list = list(grouped_dataframe[dates_list[date_index]].index)\n",
    "\n",
    "        for id_index in range(len(ids_list)):\n",
    "            json_file[main_object][date_index]['data'].append(dict())\n",
    "            json_file[main_object][date_index]['data'][id_index]['id'] = int(ids_list[id_index])\n",
    "            json_file[main_object][date_index]['data'][id_index]['nbr'] = int(grouped_dataframe[(dates_list[date_index],\n",
    "                                                                                                 ids_list[id_index])])\n",
    "            \n",
    "    with open(output_filename, 'w') as file:\n",
    "        json.dump(json_file, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "create_json_file('c', grouped_year_canton, '../res/density/canton_density.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_json_file('m', grouped_year_town, '../res/density/municipality_density.json')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}